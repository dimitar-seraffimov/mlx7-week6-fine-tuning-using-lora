{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNZSY4EyqB+WxkIgNm9QgHK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# mount my Google Drive to save the notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZCgLLn4VGekJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install libraries\n",
        "!pip install -q diffusers transformers accelerate peft bitsandbytes kaggle"
      ],
      "metadata": {
        "id": "MRg3Z94TSO0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kaggle API\n",
        "from google.colab import files\n",
        "files.upload() # upload kaggle.json API key\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "wqPUvQqkT6xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download Tom and Jerry dataset\n",
        "!kaggle datasets download -d balabaskar/tom-and-jerry-image-classification -p /content/tom_and_jerry"
      ],
      "metadata": {
        "id": "oxh9UVwaT_6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip the dataset\n",
        "!unzip -q /content/tom_and_jerry/tom-and-jerry-image-classification.zip -d /content/tom_and_jerry_dataset"
      ],
      "metadata": {
        "id": "eF89fkcKUde9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect dataset to verify and start data preparation\n",
        "import os\n",
        "dataset_path = '/content/tom_and_jerry_dataset/tom_and_jerry/tom_and_jerry'\n",
        "print(os.listdir(dataset_path))"
      ],
      "metadata": {
        "id": "UR8y_V1FVMpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get all images from the dataset\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "image_paths = []\n",
        "style_data_root = '/content/tom_and_jerry_dataset/tom_and_jerry/tom_and_jerry/'\n",
        "\n",
        "# collect images from all subfolders -> goes through all(files and folders) in the style_data_root directory\n",
        "for folder_name in os.listdir(style_data_root):\n",
        "  folder_path = os.path.join(style_data_root, folder_name)\n",
        "  if os.path.isdir(folder_path):\n",
        "    # add all files to image_paths list\n",
        "    image_paths.extend(glob.glob(os.path.join(folder_path, '*.jpg')))\n",
        "    image_paths.extend(glob.glob(os.path.join(folder_path, '*.png'))) # just in case but I think its only .jpg files\n",
        "print(f\"Found  {len(image_paths)} images to use for training.\")"
      ],
      "metadata": {
        "id": "rhwlSf2AVTQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the base model = Stable Diffusion v1.5\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "device = \"cuda\"\n",
        "\n",
        "# load pipeline with 4-bit quantization\n",
        "pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16, # try full 32bits precision later -> or rent a better GPU\n",
        "    load_in_4bit=True, # model's weights and biases are loaded and stored using only 4 bits per value\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "pipeline.to(device)\n",
        "\n",
        "pipeline.text_encoder.to(dtype=torch.float16, device=device)\n",
        "pipeline.vae.to(dtype=torch.float16, device=device)\n",
        "\n",
        "print(\"Base model loaded.\")"
      ],
      "metadata": {
        "id": "XRFQ8ls7ZLYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure LoRA for UNet\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32, # often 2*r\n",
        "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\", \"proj_in\", \"proj_out\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "# add LoRA adapters to the UNet -> making the UNet trainable, not the whole pipeline\n",
        "unet = pipeline.unet\n",
        "unet_lora = get_peft_model(unet, lora_config)\n",
        "# verify LoRA application\n",
        "unet_lora.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "RFlSDn7pht-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training script\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from accelerate import Accelerator\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# define accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "# image transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5]),\n",
        "])\n",
        "\n",
        "# custom 'Tom and Jerry' dataset class\n",
        "class TomAndJerryDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, image_paths, transform=None):\n",
        "    self.image_paths = image_paths\n",
        "    self.transform = transform\n",
        "  def __len__(self):\n",
        "    return len(self.image_paths)\n",
        "  def __getitem__(self, idx):\n",
        "    image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    return image"
      ],
      "metadata": {
        "id": "vgvu5RonmHoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training parameters\n",
        "num_epochs = 6\n",
        "learning_rate = 1e-4\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "# prepare custom 'Tom and Jerry' dataset and dataloader\n",
        "dataset = TomAndJerryDataset(image_paths, transform)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# setup optimizer for LoRA params only\n",
        "optimizer = torch.optim.AdamW(unet_lora.parameters(), lr=learning_rate)\n",
        "\n",
        "# prepare everything with accelerator\n",
        "unet_lora, optimizer, dataloader = accelerator.prepare(unet_lora, optimizer, dataloader)"
      ],
      "metadata": {
        "id": "ohDQL8afkvOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "\n",
        "print(torch.cuda.memory_summary())"
      ],
      "metadata": {
        "id": "IC2bmyddtjLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "unet_lora.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  total_loss = 0.0\n",
        "  progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "  for images in progress_bar:\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # generate latent representations\n",
        "    latents = pipeline.vae.encode(images.to(device, dtype=torch.float16)).latent_dist.sample()\n",
        "    latents = latents * 0.18215 # stable factor from Stable Diffusion\n",
        "\n",
        "    # sample random noise\n",
        "    noise = torch.randn_like(latents).to(device)\n",
        "    timesteps = torch.randint(0, pipeline.scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n",
        "\n",
        "    noisy_latents = pipeline.scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "    # predict noise with UNet-LoRA\n",
        "    noise_pred = unet_lora(noisy_latents, timesteps, encoder_hidden_states=pipeline.text_encoder(torch.zeros((latents.shape[0], 77), dtype=torch.long, device=device))[0]).sample\n",
        "\n",
        "    # compute loss - MSE between predicted and true noise\n",
        "    loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
        "    accelerator.backward(loss)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "  avg_loss = total_loss / len(dataloader)\n",
        "  print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# save fine-tuned model\n",
        "unet_lora.save_pretrained(\"/content/drive/MyDrive/MLX/fine_tuned_tom_and_jerry\")"
      ],
      "metadata": {
        "id": "dP4HgfMWk8S6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMyYqNWrTMwdgbMmtRTz2DL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# mount my Google Drive to save the notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZCgLLn4VGekJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install libraries\n",
        "!pip install -q diffusers transformers accelerate peft bitsandbytes kaggle"
      ],
      "metadata": {
        "id": "MRg3Z94TSO0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kaggle API\n",
        "from google.colab import files\n",
        "files.upload() # upload kaggle.json API key\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "wqPUvQqkT6xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download Tom and Jerry dataset\n",
        "!kaggle datasets download -d balabaskar/tom-and-jerry-image-classification -p /content/tom_and_jerry"
      ],
      "metadata": {
        "id": "oxh9UVwaT_6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip the dataset\n",
        "!unzip -q /content/tom_and_jerry/tom-and-jerry-image-classification.zip -d /content/tom_and_jerry_dataset"
      ],
      "metadata": {
        "id": "eF89fkcKUde9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect dataset to verify and start data preparation\n",
        "import os\n",
        "dataset_path = '/content/tom_and_jerry_dataset/tom_and_jerry/tom_and_jerry'\n",
        "print(os.listdir(dataset_path))"
      ],
      "metadata": {
        "id": "UR8y_V1FVMpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get all images from the dataset\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "image_paths = []\n",
        "style_data_root = '/content/tom_and_jerry_dataset/tom_and_jerry/tom_and_jerry/'\n",
        "\n",
        "# collect images from all subfolders -> goes through all(files and folders) in the style_data_root directory\n",
        "for folder_name in os.listdir(style_data_root):\n",
        "  folder_path = os.path.join(style_data_root, folder_name)\n",
        "  if os.path.isdir(folder_path):\n",
        "    # add all files to image_paths list\n",
        "    image_paths.extend(glob.glob(os.path.join(folder_path, '*.jpg')))\n",
        "    image_paths.extend(glob.glob(os.path.join(folder_path, '*.png'))) # just in case but I think its only .jpg files\n",
        "print(f\"Found  {len(image_paths)} images to use for training.\")"
      ],
      "metadata": {
        "id": "rhwlSf2AVTQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the base model = Stable Diffusion v1.5\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "device = \"cuda\"\n",
        "\n",
        "# load pipeline with 4-bit quantization\n",
        "pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16, # try full 32bits precision later -> or rent a better GPU\n",
        "    load_in_4bit=True, # model's weights and biases are loaded and stored using only 4 bits per value\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "pipeline.to(device)\n",
        "\n",
        "pipeline.text_encoder.to(dtype=torch.float16, device=device)\n",
        "pipeline.vae.to(dtype=torch.float16, device=device)\n",
        "\n",
        "print(\"Base model loaded.\")"
      ],
      "metadata": {
        "id": "XRFQ8ls7ZLYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure LoRA for UNet\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32, # often 2*r\n",
        "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\", \"proj_in\", \"proj_out\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "# add LoRA adapters to the UNet -> making the UNet trainable, not the whole pipeline\n",
        "unet = pipeline.unet\n",
        "unet_lora = get_peft_model(unet, lora_config)\n",
        "# verify LoRA application\n",
        "unet_lora.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "RFlSDn7pht-M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}